{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic: Machine Learning from Disaster\n### Jupyter notebook author: Tao Shan\n1. [prepare data](#1) \n2. [explore data](#2)\n3. [null value and data engineering](#3)\n4. [predicting model and submit solution](#4)\n\n>Hello. This is my first notebook. I'm a beginner to the competition.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n1. [prepare data]() ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import GridSearchCV, cross_val_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import data\ndt_train = pd.read_csv('/kaggle/input/titanic/train.csv')#training set\ndt_test = pd.read_csv('/kaggle/input/titanic/test.csv')#test set\ndt_all = pd.concat([dt_train, dt_test], sort = True).reset_index(drop = True)#together\ndt_combine = [dt_train, dt_test]\nprint('Training data shape = {}'.format(dt_train.shape))\nprint('Testing data shape = {}'.format(dt_test.shape))\nprint('ALL data shape = {}'.format(dt_all.shape))\nprint('Training data column = {}'.format(dt_train.columns.tolist()))\nprint('Testing data column = {}'.format(dt_test.columns.tolist()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train data has 891 rows, test data has 418 rows\nprint(dt_train.info())#training dataset infomation\ndt_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dt_test.info())#training dataset infomation\ndt_test.sample(5)#5 random sample","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n2. [explore data]()","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### We need to know what does these columns mean\n#### (according to the sequence)\n- PassengerId and Survived are ignored, Since PassengerId is useless for prediction, Survived is our target.\n- Pclass: 1 2 3\n- Name: includes name title Mr, Mrs, Miss, Master, ...\n- Sex: male or female\n- Age: numerical value between 0-100\n- Sibsp & Parch: number of sibling and parents, numerical value (probably integer between 0-10)\n- Ticket: Random number and letters\n- Fare: Price for tickets, float value\n- Cabin: Start with a letter, probably determine which group of cabin, then number for the seat\n- Embarked: Where user boarded, such as 'S', 'C'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#only three graphs, ctrl + c, ctrl + v is faster\nfig, axs = plt.subplots(ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5, top=1.25)\nplt.subplot(2,2,1)\nsns.countplot(x = 'Pclass', hue = 'Survived', data = dt_train)\nplt.subplot(2,2,2)\nsns.countplot(x = 'Sex', hue = 'Survived', data = dt_train)\nplt.subplot(2,2,3)\nsns.countplot(x = 'Embarked', hue = 'Survived', data = dt_train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### You can make some graphs to show the relation between variables and people's death, for varaiable Pclass, Sex, Embarked, since they are well grouped \nFor example, From the graph above, precentage of male dead much more than female.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# explore correlations between existing variables\n# the correlations only includes non-categorical variables\nsns.heatmap(dt_all.corr(), annot = True)\ndt_all.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For other graph other than Pclass, Sex, Embarked, I will draw in the next part, together with data engineering\nThen, we need to explore null value, then group and do some feature engineering works with Name, Age, Sibsp&Parch, Fare, Cabin","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n3. [null value and data engineering]()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of missing value(you can try feature engineering for categorical data before missing value)\ndt_all.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now do the feature engineering of Name, Age, Sibsp&Parch, Fare, Cabin. Fare, Cabin and Age have missing values, so we need to ignore them now. After feature engineering, then fill the values according with correlations, or other methods.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"3.1 data engineering\n\n3.1.1. Name","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#set expand to false. If True, return DataFrame with one column per capture group.\ndt_all['Name_Title'] = dt_all.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_title = dt_all ['Name_Title'].unique().tolist()\nprint(unique_title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Name_Title', hue='Survived', data=dt_all)\n\nplt.xlabel('Name_Title', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So Mr and Mrs has similar survival rate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all['Name_Title'].value_counts(normalize = True) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bundle rare salutations: 'Other' category\ndel unique_title[0:4]\ndt_all['Name_Title'] = dt_all['Name_Title'].replace(unique_title, 'Other')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all['Name_Title'].value_counts(normalize = True) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_category = {'Mr':1, 'Miss':2, 'Mrs':2, 'Master':3, 'Other':4}\ndt_all['Name_Title'] = dt_all['Name_Title'].map(new_category)\ndt_all['Name_Title'].unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.1.1.2 group by family","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all['family'] = dt_all.Name.str.extract('([A-Za-z]+)\\,', expand = False)\ndt_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(dt_all.family.unique().tolist()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.1.2.age","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#survive: green not survive: red\nsurvive = dt_train['Survived'] == 1\nsns.distplot(dt_train[survive]['Age'].dropna(), label='Survived', hist=True, color='#e74c3c')\nsns.distplot(dt_train[~survive]['Age'].dropna(), label='Survived', hist=True, color='#2ecc71')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all['Age'] = pd.qcut(dt_all['Age'], 5)\nsns.countplot(x='Age', hue='Survived', data=dt_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all['Age'].head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.1.3 sibsp & parch\n\nGroup sibsp and parch as a family.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all['Family_Size'] = dt_all['SibSp'] + dt_all['Parch'] + 1\nsns.countplot(x = 'Family_Size', hue = 'Survived', data = dt_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#by the plot above, grouping 1 as group1, 2,3,4 as group2, 5,6,7 as group3, 8,11 as group4 (since they looks similar)\nfamily_grouping = {1: 'group1', 2: 'group2', 3: 'group2', 4: 'group2', 5: 'group3', 6: 'group3', 7: 'group3', 8: 'group4', 11: 'group4'}\ndt_all['Family_Size_Grouped'] = dt_all['Family_Size'].map(family_grouping)\nsns.countplot(x = 'Family_Size_Grouped', hue = 'Survived', data = dt_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.1.4. fare\n\n(similar as age)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#survive: green not survive: red\nplt.figure(figsize=(20,5))\nsurvive = dt_train['Survived'] == 1\nsns.distplot(dt_train[survive]['Fare'].dropna(), label='Survived', hist=True, color='#e74c3c')\nsns.distplot(dt_train[~survive]['Fare'].dropna(), label='Survived', hist=True, color='#2ecc71')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill missing value before qcut or ccut. look at the data\ndt_all[dt_all['Fare'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dt_all.groupby(['Pclass','Family_Size','Name_Title']).Fare.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"med_fare = dt_all.groupby(['Pclass', 'Family_Size', 'Name_Title']).Fare.median()[3][1][1]\n#dt_all['Fare'] = dt_all['Fare'].fillna(med_fare)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\ndt_all['Fare'] = pd.qcut(dt_all['Fare'], 12)\nsns.countplot(x='Fare', hue='Survived', data=dt_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.1.5 Cabin","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# use M represent missing values\ndt_all['Cabin'] = dt_all['Cabin'].fillna('M')\ndt_all['Cabin'].unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#there might be one preson order one or more tickets. The first letter is unique. \n# Extract first letter\nimport re\n#pandas.Series.map\ndt_all['Cabin'] = dt_all['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group()) \ndt_all['Cabin'].unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(dt_all['Cabin'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since it is a series type\ndt_all['Cabin'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sort_list = ['M', 'C', 'E', 'G', 'D', 'A', 'B', 'F', 'T']\nsort_list = sorted(sort_list)\nprint(sort_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the first letter of tickets might relate to passenger class, according to daily experience.\n\ndt_all_decks = dt_all.groupby(['Cabin','Pclass']).count()['Survived']\ntotal_num_dict = {}\nfor letter in sort_list:\n    total_num = 0\n    for num in range(1,4):\n        try:\n            num_letter = dt_all_decks[letter][num]\n            total_num += num_letter\n            #print(total_num)\n            if num == 3:\n                total_num_dict[letter] = total_num\n                total_num = 0\n        except KeyError:\n            if num == 3:\n                total_num_dict[letter] = total_num\n                total_num = 0\nprint(total_num_dict)                \nprint(dt_all_decks['A'][1])\nprint(dt_all_decks)\ntype(dt_all_decks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precentage_num_dict = {}\nfor letter in sort_list:\n    lis = []\n    total = total_num_dict[letter]\n    for num in range(1,4):\n        try:\n            lis.append(dt_all_decks[letter][num]/total)\n        except KeyError:\n            lis.append(0)\n        if num == 3:\n            precentage_num_dict[letter] = lis\nprint(precentage_num_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#precentage chart\npd.DataFrame.from_dict(precentage_num_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So A,B,C,T are all in first Passenger class.\n\nD, E looks similar\n\nF,G looks similar\n\nM looks different from others\n\nSo that is why I grouping like this:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all['Cabin'] = dt_all['Cabin'].replace(['A', 'B', 'C', 'T'], 'A&B&C&T')\ndt_all['Cabin'] = dt_all['Cabin'].replace(['D', 'E'], 'D&E')\ndt_all['Cabin'] = dt_all['Cabin'].replace(['F', 'G'], 'F&G')\ndt_all['Cabin'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all.drop(['Name', 'Family_Size'],axis=1,inplace=True)\ndt_all.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.1.6 train tickets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all['Ticket_Frequency'] = dt_all.groupby('Ticket')['Ticket'].transform('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.2 convert values\n\n3.21 convert categorical value to numerical","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of missing value\ndt_all.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#label encoding, ignore NAN value\n#https://stackoverflow.com/questions/54444260/labelencoder-that-keeps-missing-values-as-nan\n#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\ndt_all = dt_all.apply(lambda series: pd.Series(\n    LabelEncoder().fit_transform(series[series.notnull()]),\n    index=series[series.notnull()].index))\n#number of missing value\ndt_all.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all[dt_all['Fare'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all[dt_all['Embarked'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(dt_all.corr(), annot = True)\ndt_all.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.22 fill values according with correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sort_age = dt_all.corr()['Age'].abs().sort_values(ascending = False)[1:]\n#sort_age_fit = sort_age[sort_age > 0.1][1:]#drop the sane one\nsort_age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sort_Embarked = dt_all.corr()['Embarked'].abs().sort_values(ascending = False)[1:]\n#sort_Embarked_fit = sort_Embarked[sort_Embarked > 0.1][1:]\nsort_Embarked","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sort_Fare = dt_all.corr()['Fare'].abs().sort_values(ascending = False)[1:]\n#sort_Fare_fit = sort_Fare[sort_Fare > 0.1][1:]\nsort_Fare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sort_age_name = sort_age.index.tolist()\nsort_Embarked_name = sort_Embarked.index.tolist()\nsort_Fare_name = sort_Fare.index.tolist()\nsort_lis = [sort_age_name,sort_Embarked_name, sort_Fare_name ]\nfor i in range(0,3):\n    try:\n        sort_lis[i].remove('Survived')\n        sort_lis[i].remove('PassengerId')\n    except:\n        sort_lis[i] = sort_lis[i]\nprint('Age: ',sort_age_name)\nprint('Embarked: ',sort_Embarked_name)\nprint('Fare: ',sort_Fare_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sort_Embarked_name.remove('Age')\ndt_all['Fare'] = dt_all.groupby(sort_Fare_name[0])['Fare'].apply(lambda x: x.fillna(x.median()))\ndt_all['Embarked'] = dt_all.groupby(sort_Embarked_name[0:2])['Embarked'].apply(lambda x: x.fillna(x.median()))\ndt_all['Age'] = dt_all.groupby(['Sex',sort_age_name[0]])['Age'].apply(lambda x: x.fillna(x.median()))\ndt_all.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"if you search from internet, embark should be 'S'(here is 2), just have a check","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fare_null = dt_all.loc[dt_all['PassengerId'] == 1043]\nembark_null_1 = dt_all.loc[dt_all['PassengerId'] == 61]\nembark_null_2 = dt_all.loc[dt_all['PassengerId'] == 829]\nprint(fare_null)\nprint(embark_null_1)\nprint(embark_null_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sort_Survived = dt_all.corr()['Survived'].abs().sort_values(ascending = False)[1:]\n#sort_Fare_fit = sort_Fare[sort_Fare > 0.1][1:]\nsort_Fare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all.drop(['SibSp','Parch','family'], axis = 1, inplace = True)\ndt_all.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.3 OnehogEncoder\n\nI will use it in 4.3, 4.1 and 4.2 are using labelEncoder","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"OHEncoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nsurvive_all = dt_all[['Survived','PassengerId']]\ndt_all_copy = dt_all.copy()\ndt_all_copy = dt_all_copy.drop(['Survived','PassengerId'], axis = 1)\ndt_all_copy_column = dt_all_copy.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"low_cardinality_cols = [col for col in dt_all_copy_column if dt_all_copy[col].nunique() < 5]\nhigh_cardinality_cols = [col for col in dt_all_copy_column if dt_all_copy[col].nunique() >= 5]\nprint(low_cardinality_cols,high_cardinality_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OH_cols_all = pd.DataFrame(OHEncoder.fit_transform(dt_all_copy[low_cardinality_cols]))\ndt_all_onehog = pd.concat([OH_cols_all,survive_all,dt_all_copy[high_cardinality_cols]], axis = 1)\nprint(dt_all_onehog.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_all_onehog.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now finishs feature engineering and null values part. \n<a id=\"4\"></a>\n\n4. model and prediction\n\nJust use a easy one, later I will improve it.\n\ntry the process for split data, make prediction model, sample score, and submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_train_onehog = dt_all_onehog.loc[:890]\ndt_test_onehog = dt_all_onehog.loc[891:]\ndt_test_onehog.drop(['Survived'], axis = 1, inplace = True)\ndt_train_onehog.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X_onehog = StandardScaler().fit_transform(dt_train_onehog.drop(['Survived', 'PassengerId'],axis = 1))\ntrain_y_onehog = dt_train_onehog['Survived']\ntest_X_onehog = StandardScaler().fit_transform(dt_test_onehog.drop(['PassengerId'], axis = 1))\nprint(type(train_X_onehog))\nprint('train_X shape: {}'.format(train_X_onehog.shape))\nprint('train_y shape: {}'.format(train_y_onehog.shape))\nprint('test_X shape: {}'.format(test_X_onehog.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save\npd.DataFrame(train_X_onehog).to_csv('train_X.csv', header=True, index=False)\ntrain_y_onehog.to_csv('train_y.csv', header=True, index=False)\npd.DataFrame(test_X_onehog).to_csv('test_X.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_classifier = RandomForestClassifier(random_state=42)\n'''\n#this runs for a long time\nforest_parameter = [\n    {'max_depth':[2,4,7,8,12,16,20],\n    'min_samples_split':[2,4,6,8,10],\n    'min_samples_leaf':[2,4,6,8,10],\n    'n_estimators':range(50,2000,50)}\n]\n'''\nforest_parameter = [\n    {'max_depth':[6],\n    'min_samples_split':[6],\n    'min_samples_leaf':[6],\n    'n_estimators':range(50,2000,50)}]\nramdom_forest_model = GridSearchCV(forest_classifier, forest_parameter, cv=5, verbose=1, n_jobs=-1)\nramdom_forest_model.fit(train_X_onehog, train_y_onehog)\nforest_best = ramdom_forest_model.best_estimator_\nforest_best.fit(train_X_onehog, train_y_onehog)\nforest_best_prediction = cross_val_predict(forest_best, train_X_onehog, train_y_onehog, cv=10)\nforest_best_accuracy_score = accuracy_score(train_y_onehog, forest_best_prediction)\nprint(forest_best_accuracy_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_test_prediction = forest_best.predict(test_X_onehog)\nforest_test_prediction = forest_test_prediction.astype(int)\nrandom_forest_new_submission = pd.DataFrame(columns=['PassengerId', 'Survived'])\nrandom_forest_new_submission['PassengerId'] = dt_test_onehog['PassengerId'].add(1).to_numpy()\nrandom_forest_new_submission['Survived'] = forest_test_prediction\nrandom_forest_new_submission.to_csv('forest_submission_20200602.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_new_submission.tail(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score: 0.79904","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Learning materials: \nhttps://www.kaggle.com/learn/intro-to-machine-learning\n\nhttps://www.kaggle.com/learn/intermediate-machine-learning\n\nhttps://www.kaggle.com/learn/data-visualization\n\nhttps://www.kaggle.com/learn/feature-engineering\n\nhttps://www.kaggle.com/kernelgenerator/titanic-tutorial-for-beginners-part-1\n\nhttps://www.kaggle.com/kernelgenerator/titanic-tutorial-for-beginners-part-2\n\nhttps://www.kaggle.com/kernelgenerator/titanic-tutorial-for-beginners-part-3\n\nhttps://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial#2.-Feature-Engineering![image.png](attachment:image.png)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}